---
title: "An√°lisis de S√≥lidos Suspendidos"
format: 
  html:
    number-sections: true
    toc: true 
    toc-location: right
    toc-depth: 3
    embed-resources: true
    crossrefs-hover: false
    lang: es
    bibliography: bibliografia/bibliografia.bib
    csl: bibliografia/ieee.csl
date: last-modified
author:
  - name: Ariadna Malena Seba
    orcid: 
    corresponding: true
    email: ariadna.mseba@ca.frre.utn.edu.ar
    affiliations:
      - name: GISTAQ (UTN-FRRe)
        url: https://www.instagram.com/gistaq.utn/
abstract: |
  Este sitio web contiene cuestiones etc
keywords:
  - GISTAQ
  - UTN
  - FRRe
  - Quarto
jupyter: python3
execute:
  echo: true
---

## S√≥lidos suspendidos totales<span style="font-weight:normal; font-size: 1rem">, por Vera Geneyer (https://github.com/VeraGeneyer)</span> {toc-text="S√≥lidos suspendidos totales"}

Los s√≥lidos suspendidos totales (TSM): es la cantidad de materia en suspensi√≥n en el agua, que incluye plancton, minerales, arena, y microorganismos. Se determinan como el residuo no filtrable de una muestra de agua. Niveles altos (TSM) pueden reducir la transparencia del agua, limitar la luz y y transportar sustancias t√≥xicas, afectando la vida acu√°tica y la calidad del agua.
Este par√°metro, medido mediante sensores remotos, nos da informaci√≥n sobre el estado f√≠sico del cuerpo de agua y est√°n relacionados con factores como la humedad, temperatura y entre otros, que es vital para detectar riesgos al ecosistema y cumplir con las normas ambientales.

### M√©todos tradicionales

:::: {.content-visible when-format="html"}

::: {.column-screen-right}

| Ecuaci√≥n | Bandas (nm) | M√©tricas | Aguas | Plataforma | Referencia |
|:-:|:--|:--|:--|:--|:-:|
| $-229.34 \left( \frac{B03}{B08} \right)^{3}+1001.65 \left( \frac{B03}{B08} \right)^{2}-1422.7 \left( \frac{B03}{B08} \right)+665.17$ | B03, B08 | $R^{2}$ | Embalse^[Aguas l√©nticas.] | Landsat-8 | @Ramirez2017 |
| $-244.83+40.21 \cdot B01-3.67 \cdot NDWI$ | B01, NDWI (B03, B08) | $R^{2}$, RMSE, d | R√≠o^[d = prueba estad√≠stica de <b>Durbin-Watson</b>.] | GeoEye | @Gomez2014 |

: Caracter√≠sticas principales de algoritmos tradicionales para la estimaci√≥n de s√≥lidos suspendidos. {#tbl-solsus-trad .striped .hover tbl-colwidths="[50,10,10,10,10]"}

:::

::::

:::: {.content-visible when-format="typst"}

| Ecuaci√≥n | Referencia |
|:--|:-:|
| $-229.34 \left( \frac{B03}{B08} \right)^{3}+1001.65 \left( \frac{B03}{B08} \right)^{2}-1422.7 \left( \frac{B03}{B08} \right)+665.17$ | [@Ramirez2017] |
| $-244.83+40.21 \cdot B01-3.67 \cdot NDWI$ | [@Gomez2014] |

: Caracter√≠sticas principales de algoritmos tradicionales para la estimaci√≥n de s√≥lidos suspendidos. {#tbl-solsus-trad .striped .hover tbl-colwidths="[80,20]"}

::: {.block stroke='rgb("#B86092")' inset="8pt" radius="4pt"}

[Ver tabla completa en la versi√≥n online &#x2197;](https://vhgauto.quarto.pub/gistaq-parana/#tbl-solsus-trad)

:::

::::

De acuerdo a un estudio que analiz√≥ 48 cuerpos de agua, la estimaci√≥n de TSM se hizo en su mayor√≠a por modelos lineales, siendo la banda B8A la m√°s frecuente [@Cruz2023].

### M√©todos de aprendizaje autom√°tico

El **aprendizaje autom√°tico (ML)**  es una rama de la inteligencia artificial cuyo objetivo es desarrollar algoritmos capaces de resolver problemas mediante el an√°lisis de datos y la creaci√≥n de funciones que describen el comportamiento de fen√≥menos monitoreados [@Carpio2021]. Los modelos de aprendizaje autom√°tico m√°s utilizados y mencionados por los investigadores para predecir la concentraci√≥n de SST son:

* **Bosque Aleatorio (RF) y Refuerzo Adaptativo (AdB)**, modelos que se destacan por su robustez ante datos complejos y ruidosos. Estos algoritmos construyen m√∫ltiples √°rboles de decisi√≥n que analizan las relaciones entre caracter√≠sticas como el uso del suelo o el volumen de escorrent√≠a y los niveles de SST [@Moeini2021].

* **Redes Neuronales Artificiales (ANN)**, copian las redes neuronales biol√≥gicas y aprenden patrones complejos en grandes vol√∫menes de datos, como los niveles de SST en distintas condiciones ambientales [@Moeini2021],

* **k-Nearest Neighbors (kNN)**, en sus variantes de ponderaci√≥n uniforme y variable, que estima el SST en funci√≥n de la cercan√≠a en caracter√≠sticas de nuevos puntos de muestreo con datos hist√≥ricos [@Moeini2021].

El aprendizaje autom√°tico es esencial para mejorar la precisi√≥n y rapidez en el an√°lisis de la calidad del agua, proporcionando un monitoreo m√°s eficiente y menos costoso en comparaci√≥n con los m√©todos tradicionales, especialmente en √°reas de dif√≠cil acceso o con datos limitados.

:::: {.content-visible when-format="html"}

::: {.column-screen-right}

| **Modelo de machine learning** | **Software** | **Agua** | **Datos** | **M√©tricas** | **Referencias** |
|:--|:--|:--|:--|:--|:-:|
|Bagging y Random Forest|Programa R|Bah√≠a|Muestreo|Prueba de normalidad multivalente Mardia-tests y Royston|@Carpio2021|
|Regresi√≥n lineal, LASSO, regresi√≥n de vectores de soporte (SVR), K vecinos m√°s cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).|-|Lago y embalse|Sentinel-2 y UAV|$R^{2}$| @Silveira2020|
|Regresi√≥n lineal, regresi√≥n de vectores de soporte (SVR), K vecinos m√°s cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).|Programa Python|Lagos|Estaci√≥n de monitoreo (Sensores para cada par√°metro)|$R^{2}$, NSE y RMSE| @Moeini2021|

: Caracter√≠sticas principales de algoritmos de aprendizaje autom√°tico para la estimaci√≥n de s√≥lidos suspendidos. {#tbl-solsus-machine .striped .hover tbl-colwidths="[40,12,12,13,13,10]"}

:::

::::

:::: {.content-visible when-format="typst"}

| Modelo de machine learning | Referencias |
|:--|:-:|
|Bagging y Random Forest| [@Carpio2021] |
|Regresi√≥n lineal, LASSO, regresi√≥n de vectores de soporte (SVR), K vecinos m√°s cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).| [@Silveira2020] |
|Regresi√≥n lineal, regresi√≥n de vectores de soporte (SVR), K vecinos m√°s cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).| [@Moeini2021] |

: Caracter√≠sticas principales de algoritmos de aprendizaje autom√°tico para la estimaci√≥n de s√≥lidos suspendidos. {#tbl-solsus-machine .striped .hover tbl-colwidths="[80,20]"}

::: {.block stroke='rgb("#B86092")' inset="8pt" radius="4pt"}

[Ver tabla completa en la versi√≥n online &#x2197;](https://vhgauto.quarto.pub/gistaq-parana/#tbl-solsus-machine)

:::

::::




## **Python**, con `sklearn`


### Procesamiento de datos

Se detalla el procedimiento t√©cnico que implement√© para procesar informaci√≥n ambiental georreferenciada con el objetivo de analizar el comportamiento del par√°metro **s√≥lidos suspendidos (sol_sus)** en una regi√≥n espec√≠fica (pixel `3x3`). Para esto, utilic√© el lenguaje Python y la biblioteca `pandas`, que resulta particularmente eficiente para el manejo de estructuras tabulares. 

#### Carga de datos

Primero importo la biblioteca `pandas`, una herramienta en Python que se utiliza para manejar datos en formato tabular (como hojas de c√°lculo o CSVs). Se le da el alias `pd` por convenci√≥n, para simplificar el c√≥digo.

Luego cargo dos archivos CSV con la funci√≥n `pd.read_csv()`, la cual convierte dichos archivos en objetos del tipo `DataFrame`, que representan tablas en memoria, que son estructuras de datos similares a tablas (parecida a una hoja de Excel). Los conjuntos de datos cargados fueron:

- `gis_df`: contiene informaci√≥n geogr√°fica (latitud, longitud, pixel, etc.).
- `lab_df`: contiene datos de laboratorio, incluyendo el par√°metro de inter√©s `sol_sus`.

 Verifico la carga correcta mostrando las primeras filas con la funci√≥n `.head()`. Es √∫til para ver r√°pidamente c√≥mo es la estructura del archivo: qu√© columnas hay, qu√© tipo de datos, si se carg√≥ bien.

```{python}
import pandas as pd  # pandas es la biblioteca para manejar datos tabulares

# Cargar los archivos de datos
gis_df = pd.read_csv('datos/base_de_datos_gis.csv')
lab_df = pd.read_csv('datos/base_de_datos_lab.csv')

# Ver las primeras filas para asegurarse de que se cargaron bien
gis_df.head(), lab_df.head()

print("Primeras filas de gis_df:")
display(gis_df.head())

print("\nPrimeras filas de lab_df:")
display(lab_df.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- `pd.read_csv()` carga los archivos en estructuras llamadas *dataframes*, que funcionan como tablas.  
- `head()` te muestra las primeras 5 filas para ver c√≥mo est√°n los datos.
- `display()` permite mostrar las tablas con formato m√°s visual (en HTML).

</details>

:::

#### Filtrar el par√°metro 'sol_sus'

En el conjunto de datos del laboratorio `lab_df`, hay m√∫ltiples par√°metros medidos (como pH, turbidez, etc.). En este caso, me interesa trabajar solamente con los datos de **s√≥lidos suspendidos**, identificado como `"sol_sus"` en la columna `param`. Este filtrado selectivo lo realic√© para limitar el an√°lisis al fen√≥meno f√≠sico-qu√≠mico de inter√©s.

Filtr√© el DataFrame para quedarme solo con esas filas, y renombr√© la columna `valor` como `sol_sus` para que sea m√°s claro en los siguientes pasos.

```{python}
# Filtramos solo las filas donde el par√°metro es 'sol_sus'
sol_sus_df = lab_df[lab_df["param"] == "sol_sus"]

# Renombramos la columna 'valor' a 'sol_sus' para que tenga sentido en el merge
sol_sus_df = sol_sus_df.rename(columns={"valor": "sol_sus"})

# Mostramos para confirmar
sol_sus_df.head()
print("Primeras filas de sol_sus_df:")
display(sol_sus_df.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- `lab_df[lab_df["param"] == "sol_sus"]` filtra las filas cuyo valor en la columna `"param"` sea `"sol_sus"`.  
- `.rename(columns={"valor": "sol_sus"})` cambia el nombre de la columna `"valor"` a `"sol_sus"`.

</details>

:::


#### Transformar la columna banda en columnas individuales

En este paso, convierto los valores √∫nicos de la columna `banda` (como `B01`, `B02`, etc.) en nombres de columnas. Cada nueva columna contendr√° los valores del par√°metro `reflect` correspondientes a esa banda en particular. Esta operaci√≥n se realiza antes de unir con los valores de `sol_sus`, ya que el valor de reflectancia depende de la banda, mientras que `sol_sus` es un dato independiente que se asignar√° luego por punto, fecha y ubicaci√≥n.

```{python}
# Pivotamos la tabla para que cada banda sea una columna
gis_pivot = gis_df.pivot_table(
    index=['fecha', 'punto', 'pixel', 'latitud', 'longitud'],
    columns='banda',
    values='reflect'
).reset_index()

# Eliminamos el nombre del √≠ndice de columnas generado por el pivot
gis_pivot.columns.name = None

print("Primeras filas de gis_pivot:")
display(gis_pivot.head())
```

::: {.dropdown}
<details> 
<summary>üìÑ Nota t√©cnica</summary>

- `pivot_table()` reorganiza el DataFrame convirtiendo los valores de una columna (`banda`) en columnas individuales.

- `index=[...]` define las columnas que se mantendr√°n como claves (se repetir√°n por fila).

- `columns='banda'` indica qu√© columna queremos transformar en nombres de columnas.

- `values='reflect'` especifica qu√© valor colocar en cada celda de la nueva tabla.

- `reset_index()` convierte los √≠ndices jer√°rquicos en columnas normales para facilitar el an√°lisis.

- `columns.name = None` quita la etiqueta "banda" que se agregar√≠a al encabezado por defecto.

</details> 

:::


#### Combinar datos geoespaciales y de laboratorio

Una vez que las bandas han sido transformadas en columnas, combino esta tabla con los valores de s√≥lidos suspendidos (`sol_sus`) provenientes del laboratorio. La combinaci√≥n se hace usando las columnas `fecha`, `latitud` y `longitud`, que permiten identificar los datos correspondientes a un mismo punto geogr√°fico y temporal.

```{python}
# Realizamos el merge por ubicaci√≥n y fecha
df_merged = pd.merge(
    gis_pivot,
    sol_sus_df[['fecha', 'latitud', 'longitud', 'sol_sus']],
    on=['fecha', 'latitud', 'longitud'],
    how='inner'
)

print("Primeras filas del DataFrame combinado:")
display(df_merged.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- `pd.merge()` permite combinar dos DataFrames en uno nuevo, uniendo filas que coincidan en las columnas especificadas.  
- `on=["latitud", "longitud"]` indica que la combinaci√≥n debe hacerse usando esas columnas como claves.  
- `how="inner"` especifica el tipo de combinaci√≥n:  
  - `"inner"`: solo conserva las filas donde hay coincidencia en ambos DataFrames.  
  - Otras opciones:  
    - `"left"`: conserva todas las filas del primer DataFrame.  
    - `"right"`: conserva todas las filas del segundo.  
    - `"outer"`: conserva todo, incluso si no hay coincidencia.

</details>

:::

#### Filtrado espacial por pixel

Luego de combinar los datos, aplico un filtrado adicional al DataFrame sobre la columna `pixel` para conservar √∫nicamente las filas correspondientes al √°rea geogr√°fica designada como `"3x3"`. Este paso reduce el dominio de an√°lisis y permite concentrarse en una regi√≥n de estudio concreta.

```{python}
# Filtramos solo los datos del pixel 3x3
df_pixel_3x3 = df_merged[df_merged["pixel"] == "3x3"]

print("Primeras filas del pixel 3x3:")
display(df_pixel_3x3.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- Usa filtrado booleano (`DataFrame[condici√≥n]`), que es la forma est√°ndar en pandas para seleccionar subconjuntos de datos. 
- `df_pixel_3x3 = df_combinado[df_combinado["pixel"] == "3x3"]` selecciona ese subconjunto. Filtra las filas cuyo valor en la columna `"pixel"` es igual a `"3x3"`.

</details>

:::


#### Guardar el archivo final

Finalmente, guardo el resultado como un nuevo archivo .csv dentro de la carpeta datos. 

Por √∫ltimo, exporto el resultado a un nuevo archivo en formato `.csv`, mediante la funci√≥n `to_csv()` de pandas, con el par√°metro `index=False` para evitar que la columna de √≠ndice se incluya en el archivo de salida que pandas crea por defecto.
Esto me permite utilizarlo despu√©s para visualizaci√≥n o an√°lisis posterior.

```{python}
# Guardar el archivo CSV dentro de la carpeta "datos"
df_pixel_3x3.to_csv('datos/datos_sol_sus_pixel_3x3.csv', index=False)
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- `to_csv()`  guarda los datos en formato CSV.  
- `index=False` evita que se guarde el √≠ndice num√©rico del DataFrame como una columna adicional en el CSV.

</details>

:::



### An√°lisis de Regresi√≥n Lineal

En este an√°lisis aplico un modelo de regresi√≥n lineal simple para estudiar la relaci√≥n entre la **reflectancia** y los **s√≥lidos suspendidos**, utilizando datos experimentales. La regresi√≥n lineal es una t√©cnica fundamental del aprendizaje autom√°tico supervisado que nos permite predecir un valor continuo basado en una o m√°s variables independientes. A lo largo de este documento, se explican paso a paso las acciones realizadas y los conceptos clave para comprender y replicar este an√°lisis.

#### Importar librer√≠as

En este paso, cargo las bibliotecas necesarias para procesar datos, ajustar modelos de regresi√≥n, evaluar su desempe√±o y visualizar los resultados. 

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `pandas` se utiliza para manejar datos en forma de tablas (DataFrames), especialmente √∫tiles al trabajar con archivos `.csv`.

> `train_test_split` permite dividir los datos en subconjuntos de entrenamiento y prueba, lo cual es esencial para evaluar el desempe√±o de un modelo sin sobreajustarlo.

> `LinearRegression` representa un modelo lineal que se ajusta a los datos minimizando el error cuadr√°tico entre las predicciones y los valores reales.

> `mean_squared_error` y `r2_score` son m√©tricas de evaluaci√≥n: el primero mide el promedio de los errores al cuadrado, mientras que el segundo indica qu√© tan bien el modelo explica la variabilidad de los datos.

> `matplotlib.pyplot` se utiliza para crear gr√°ficos. Permite visualizar los datos y los resultados del modelo.
 
</details>

:::

#### Cargar datos desde un CSV

Importo el archivo `.csv` con los datos experimentales. Se visualizan las primeras filas para verificar que los datos se han cargado correctamente.

```{python}
# Cargamos el CSV
datos = pd.read_csv('datos/datos_sol_sus_pixel_3x3.csv')

# Mostramos las primeras filas para verificar
datos.head()
print("Primeras filas de datos:")
display(datos.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `pd.read_csv` carga datos desde un archivo `.csv` y los convierte en un DataFrame de Pandas. Esta estructura tabular permite filtrar, seleccionar y transformar f√°cilmente los datos.

> `datos.head()` permite ver las primeras 5 filas del DataFrame para tener una vista preliminar de los datos cargados.

</details>

:::

#### Seleccionar variables y dividir en conjuntos

Selecciono las variables relevantes: `B01` como variable independiente y `sol_sus` como variable dependiente. Luego divido el conjunto en dos subconjuntos: uno para entrenar el modelo y otro para probarlo, lo cual sirve para evaluar su capacidad de generalizaci√≥n.

```{python}
# Selecci√≥n de variables
X = datos[["B01"]]
y = datos["sol_sus"]

# Divisi√≥n en entrenamiento y validaci√≥n
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> Se selecciona una columna como variable independiente (X) y otra como variable dependiente (y). Es importante usar doble corchete al seleccionar una sola columna como X para mantener la estructura de tabla.

> `train_test_split` divide el conjunto de datos en entrenamiento y prueba. Esto permite entrenar el modelo en un subconjunto y evaluar su capacidad de generalizaci√≥n con otro.

> El par√°metro `test_size=0.2` indica que el 20% de los datos se usan para prueba. `shuffle=False` mantiene el orden original de los datos, √∫til cuando los datos est√°n organizados temporalmente o espacialmente.

</details>

:::

#### Entrenar modelo de regresi√≥n lineal

En este paso se entrena un modelo de regresi√≥n lineal usando los datos de entrenamiento. El modelo aprende la relaci√≥n matem√°tica entre la reflectancia y los s√≥lidos suspendidos.

```{python}
regressor = LinearRegression().fit(X_train, y_train)
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `LinearRegression().fit()` ajusta un modelo lineal a los datos de entrenamiento. Internamente calcula la pendiente e intercepto que minimizan la diferencia entre las predicciones y los valores reales.

</details>

:::

#### Evaluar desempe√±o del modelo

Una vez entrenado el modelo, evaluo su desempe√±o usando m√©tricas estad√≠sticas. Estas nos permiten cuantificar qu√© tan bien el modelo predice los valores de s√≥lidos suspendidos a partir de la reflectancia en los datos de prueba.

```{python}
y_pred = regressor.predict(X_test)
p_rmse = mean_squared_error(y_test, y_pred)
p_r2 = r2_score(y_test, y_pred)

```

::: {.callout-note title="M√©tricas de desempe√±o"}

```{python}
#| echo: false

print("El error cuadr√°tico medio es:", round(p_rmse, 3))
print("El coeficiente de determinaci√≥n (R¬≤) es:", round(p_r2, 3))
```
:::

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `predict()` genera predicciones del modelo usando los datos de prueba. Estas predicciones se comparan con los valores reales para evaluar el desempe√±o.

> `mean_squared_error` calcula el promedio de los errores al cuadrado. Cuanto menor sea este valor, mejor se ajusta el modelo.

> `r2_score` mide qu√© proporci√≥n de la variabilidad en los datos es explicada por el modelo. Un valor cercano a 1 indica una buena predicci√≥n.

</details>

:::

#### Visualizar el modelo

Finalmente, se visualiza gr√°ficamente la relaci√≥n entre reflectancia y s√≥lidos suspendidos, tanto en el conjunto de entrenamiento como en el de prueba. Esto ayuda a interpretar de forma visual c√≥mo se ajusta el modelo a los datos reales.

```{python}
fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)

# Gr√°fico entrenamiento
ax[0].plot(X_train, regressor.predict(X_train), linewidth=3, color="#17A77E", label="Modelo")
ax[0].scatter(X_train, y_train, label="Entrenamiento", color="#9D50A6", alpha=0.6)
ax[0].set(xlabel="Reflectancia", ylabel="Sol_Sus", title="Conjunto de entrenamiento")
ax[0].legend()

# Gr√°fico validaci√≥n
ax[1].plot(X_test, y_pred, linewidth=3, color="#17A77E", label="Modelo")
ax[1].scatter(X_test, y_test, label="Validaci√≥n", color="#9D50A6", alpha=0.6)
ax[1].set(xlabel="Reflectancia", ylabel="Sol_Sus", title="Conjunto de validaci√≥n")
ax[1].legend()

fig.suptitle("Regresi√≥n lineal")

plt.show()
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `plt.subplots` crea una figura con uno o m√°s ejes para dibujar. Permite organizar varios gr√°ficos en una misma figura.

> `plot()` traza una l√≠nea continua. Se usa para mostrar la l√≠nea de regresi√≥n generada por el modelo.

> `scatter()` traza puntos individuales. Se usa para mostrar los datos reales y compararlos con la l√≠nea del modelo.

> `set()` configura etiquetas de ejes y t√≠tulos de los subgr√°ficos.

> `legend()` muestra una leyenda que identifica cada elemento del gr√°fico.

> `fig.suptitle()` agrega un t√≠tulo general a la figura completa.

> `plt.show()` es necesario para visualizar los gr√°ficos al renderizar el documento.

</details>

:::


### An√°lisis de regresi√≥n por banda

```{python}
#|code-fold: true
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import math

# Cargar datos
datos = pd.read_csv('datos/datos_sol_sus_pixel_3x3.csv')

# Detectar columnas de bandas
bandas = [col for col in datos.columns if col.startswith("B")]

# Lista para guardar resultados
resultados = []

# Par√°metros para organizaci√≥n de gr√°ficos
n_bandas = len(bandas)
ncols = 2  # N√∫mero de columnas de la grilla
nrows = math.ceil(n_bandas / ncols)  # Calculamos cu√°ntas filas se necesitan

# Crear figura
fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 4 * nrows))
axs = axs.flatten()  # Asegura que podamos indexarlos como una lista

for i, banda in enumerate(bandas):
    # Variables
    X = datos[[banda]]
    y = datos["sol_sus"]

    # Divisi√≥n entrenamiento/prueba
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

    # Ajuste del modelo
    modelo = LinearRegression().fit(X_train, y_train)
    y_train_pred = modelo.predict(X_train)

    # M√©tricas
    r2 = modelo.score(X_train, y_train)
    n = len(y_train)
    p = X_train.shape[1]
    r2_adj = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))

    resultados.append({
        "Banda": banda,
        "R2": round(r2, 4),
        "R2_ajustado": round(r2_adj, 4),
        "RMSE": round(rmse, 4)
    })

    # Gr√°fico de entrenamiento
    ax = axs[i]
    ax.scatter(X_train, y_train, alpha=0.6, color="#9D50A6", label="Entrenamiento")
    ax.plot(X_train, y_train_pred, color="#17A77E", linewidth=1.8, label="Modelo")
    ax.set_title(f'{banda}\nR¬≤={r2:.2f}', fontsize=10)
    ax.set_xlabel('Reflectancia', fontsize=8)
    ax.set_ylabel('Sol_Sus', fontsize=8)
    ax.tick_params(axis='both', which='major', labelsize=8)
    ax.legend(fontsize=7)
    ax.grid(True)

# Eliminar ejes sobrantes
for j in range(i + 1, len(axs)):
    fig.delaxes(axs[j])

plt.suptitle("Regresiones lineales por banda (entrenamiento)", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# Tabla resumen
df_resultados = pd.DataFrame(resultados).sort_values("R2", ascending=False).reset_index(drop=True)
print("Tabla resumen de m√©tricas por banda:")
display(df_resultados)

```


**Calidad del ajuste (R¬≤ y R¬≤ ajustado):**
- **R¬≤ bajo (<‚ÄØ0.20)** en todas las bandas, lo cual indica que **ninguna banda aislada** explica buena parte de la variabilidad de los s√≥lidos suspendidos.
- **R¬≤ ajustado** muy similar a R¬≤ en las bandas con mejor desempe√±o (B08, B07, B05, B06), y **negativo** en las bandas con R¬≤ casi nulo (B03, B02, B01, B12, B11).  
- **Un R¬≤ ajustado negativo** implica que usar la media de la variable (sin modelo) ser√≠a m√°s eficaz que la regresi√≥n lineal simple.

**Error de predicci√≥n (RMSE):**

- **RMSE ‚âà‚ÄØ25** para las cuatro mejores bandas (B08‚ÄìB06), es decir, el error medio de predicci√≥n es de unas 25 unidades de `sol_sus`.  
- Las bandas con peor R¬≤ (B11, B12) presentan **RMSE ‚âà‚ÄØ27.5**, casi equivalente a predecir siempre la media.

**Siguientes pasos recomendados:**

1. **Regresi√≥n multivariable**  
   Combinar las bandas mejor correlacionadas (por ejemplo, B08, B07, B05, B06) en un √∫nico modelo para evaluar si en conjunto mejoran R¬≤ y reducen RMSE.

2. **Modelos no lineales**  
   Probar algoritmos como **Random Forest**, **SVM** o **Redes Neuronales** para capturar relaciones no lineales o interacciones espectrales.

3. **Ingenier√≠a de variables**  
  - Calcular **√≠ndices espectrales** (NDVI, √≠ndices de turbidez) como combinaciones de bandas.  
  - Aplicar transformaciones (logaritmos, potencias) a las reflectancias o a `sol_sus` para estabilizar la varianza.

4. **Validaci√≥n cruzada**  
   Implementar **k‚Äëfold cross‚Äëvalidation** para obtener m√©tricas m√°s robustas y evitar sobreajuste.


#### An√°lisis de regresi√≥n por banda aplicando logaritmo a las variables

```{python}
#|code-fold: true
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Cargar datos
datos = pd.read_csv('datos/datos_sol_sus_pixel_3x3.csv')

# Detectar columnas de bandas
bandas = [col for col in datos.columns if col.startswith("B")]

# Lista para guardar resultados
resultados = []

for banda in bandas:
    # Variables: aplicamos logaritmo natural
    X = np.log(datos[[banda]].replace(0, np.nan)).dropna()
    y = np.log(datos.loc[X.index, "sol_sus"].replace(0, np.nan))
    
    # Divisi√≥n en entrenamiento y prueba
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=False
    )

    # Ajuste del modelo sobre los datos log-transformados
    modelo = LinearRegression().fit(X_train, y_train)
    y_train_pred = modelo.predict(X_train)

    # M√©tricas en escala logar√≠tmica
    r2 = modelo.score(X_train, y_train)
    n = len(y_train)
    p = X_train.shape[1]
    r2_adj = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))

    resultados.append({
        "Banda": banda,
        "R2_log": round(r2, 4),
        "R2_ajustado_log": round(r2_adj, 4),
        "RMSE_log": round(rmse, 4)
    })

# Convertir a DataFrame y ordenar
df_resultados = pd.DataFrame(resultados) \
    .sort_values("R2_log", ascending=False) \
    .reset_index(drop=True)

# Mostrar tabla de resultados
print("Tabla de regresi√≥n lineal sobre variables log-transformadas:")
display(df_resultados)

```

**Mejora con respecto al modelo lineal original:**

- Las bandas B05, B07, B08 y B06 muestran ahora un R¬≤ (log) entre 0.25 y 0.26, un aumento relativo frente al R¬≤ original (~0.16‚Äì0.17).
- El RMSE (log) ha disminuido hasta ~0.236 para B05, frente al RMSE promedio de ~25 en escala original (equivalente a ~ln-unidades).
- La relaci√≥n entre reflectancia y s√≥lidos suspendidos se ajusta un poco mejor tras la transformaci√≥n log‚Äìlog, sugiriendo una dependencia multiplicativa o de potencia m√°s que estrictamente lineal.

**Orden de relevancia de las bandas:**

- B05: mejor ajuste (R¬≤_log = 0.2594, RMSE_log = 0.2363).
- B07 (0.2549) y B08 (0.2500) siguen muy cerca;
- B06 completa el grupo principal (0.2481).
Este ranking coincide con el an√°lisis sin log.

**R¬≤ ajustado:**

La diferencia entre R¬≤_log y R¬≤_ajustado_log es peque√±a (<‚ÄØ0.02) para las bandas con mejor desempe√±o, lo que indica que el ajuste mejora genuinamente y no es un artefacto por el n√∫mero de variables.

**Bandas con nulo o negativo R¬≤ ajustado:**

- B11 y B12 permanecen sin aporte (R¬≤_log = 0), igual que antes.
- B02 y B01 siguen mostrando R¬≤ ajustado negativo, confirmando su irrelevancia en un modelo univariante.


### Desarrollo del modelo

Nunca una sola banda explic√≥ mucho (>‚ÄØ0.26 de R¬≤_log), pero quiz√° muchas juntas s√≠ captan gran parte de la variabilidad.


#### Incorporar AIC en la selecci√≥n de modelo

El AIC penaliza por complejidad y te ayudar√° a decidir si conviene agregar o quitar bandas.

```{python}
#|code-fold: true
import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Cargar datos
datos = pd.read_csv('datos/datos_sol_sus_pixel_3x3.csv')

# Detectar columnas de bandas
bandas = [col for col in datos.columns if col.startswith("B")]

# Lista para guardar resultados
resultados = []

for banda in bandas:
    # Log‚Äëtransform (evitando ceros)
    X = np.log(datos[[banda]].replace(0, np.nan)).dropna()
    y = np.log(datos.loc[X.index, "sol_sus"].replace(0, np.nan))
    
    # Divisi√≥n en entrenamiento/prueba
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=False
    )

    # Ajuste de OLS para obtener AIC, R¬≤ y R¬≤ ajustado
    Xc = sm.add_constant(X_train)           # a√±ade intercepto
    modelo = sm.OLS(y_train, Xc).fit()

    # Predicciones de entrenamiento para RMSE
    y_train_pred = modelo.predict(Xc)

    # M√©tricas
    r2     = modelo.rsquared
    r2_adj = modelo.rsquared_adj
    mse    = mean_squared_error(y_train, y_train_pred)
    rmse   = np.sqrt(mse)
    aic    = modelo.aic

    resultados.append({
        "Banda":        banda,
        "R2":       round(r2,     4),
        "R2_ajustado":  round(r2_adj, 4),
        "RMSE":     round(rmse,   4),
        "AIC":          round(aic,    2)
    })

# Convertir a DataFrame y ordenar por R2_log
df_resultados = (
    pd.DataFrame(resultados)
      .sort_values("R2", ascending=False)
      .reset_index(drop=True)
)

# Mostrar tabla de resultados
print("Tabla de regresi√≥n log‚Äìlog por banda con AIC (statsmodels):")
display(df_resultados)

```