---
title: "An√°lisis de S√≥lidos Suspendidos"
format: 
  html:
    number-sections: true
    toc: true 
    toc-location: right
    toc-depth: 3
    embed-resources: true
    crossrefs-hover: false
    lang: es
    bibliography: bibliografia/bibliografia.bib
    csl: bibliografia/ieee.csl
date: last-modified
author:
  - name: Ariadna Malena Seba
    orcid: 
    corresponding: true
    email: ariadna.mseba@ca.frre.utn.edu.ar
    affiliations:
      - name: GISTAQ (UTN-FRRe)
        url: https://www.instagram.com/gistaq.utn/
abstract: |
  Este sitio web aborda el an√°lisis de s√≥lidos suspendidos totales (SST) en cuerpos de agua mediante el uso de im√°genes satelitales y t√©cnicas de aprendizaje autom√°tico. Presenta una introducci√≥n te√≥rica sobre la importancia de los SST como indicador ambiental y luego desarrolla una parte pr√°ctica con Python, donde se integran datos espectrales y mediciones reales para aplicar modelos de regresi√≥n. Se eval√∫a el desempe√±o de estos modelos y se exploran relaciones entre bandas espectrales y SST, proponiendo mejoras metodol√≥gicas para estudios futuros.
keywords:
  - GISTAQ
  - UTN
  - FRRe
  - Quarto
jupyter: python3
execute:
  echo: true
---

## S√≥lidos suspendidos totales<span style="font-weight:normal; font-size: 1rem">, por Vera Geneyer (https://github.com/VeraGeneyer)</span> {toc-text="S√≥lidos suspendidos totales"}

Los s√≥lidos suspendidos totales (TSM): es la cantidad de materia en suspensi√≥n en el agua, que incluye plancton, minerales, arena, y microorganismos. Se determinan como el residuo no filtrable de una muestra de agua. Niveles altos (TSM) pueden reducir la transparencia del agua, limitar la luz y y transportar sustancias t√≥xicas, afectando la vida acu√°tica y la calidad del agua.
Este par√°metro, medido mediante sensores remotos, nos da informaci√≥n sobre el estado f√≠sico del cuerpo de agua y est√°n relacionados con factores como la humedad, temperatura y entre otros, que es vital para detectar riesgos al ecosistema y cumplir con las normas ambientales.

### M√©todos tradicionales

:::: {.content-visible when-format="html"}

::: {.column-screen-right}

| Ecuaci√≥n | Bandas (nm) | M√©tricas | Aguas | Plataforma | Referencia |
|:-:|:--|:--|:--|:--|:-:|
| $-229.34 \left( \frac{B03}{B08} \right)^{3}+1001.65 \left( \frac{B03}{B08} \right)^{2}-1422.7 \left( \frac{B03}{B08} \right)+665.17$ | B03, B08 | $R^{2}$ | Embalse^[Aguas l√©nticas.] | Landsat-8 | @Ramirez2017 |
| $-244.83+40.21 \cdot B01-3.67 \cdot NDWI$ | B01, NDWI (B03, B08) | $R^{2}$, RMSE, d | R√≠o^[d = prueba estad√≠stica de <b>Durbin-Watson</b>.] | GeoEye | @Gomez2014 |

: Caracter√≠sticas principales de algoritmos tradicionales para la estimaci√≥n de s√≥lidos suspendidos. {#tbl-solsus-trad .striped .hover tbl-colwidths="[50,10,10,10,10]"}

:::

::::

:::: {.content-visible when-format="typst"}

| Ecuaci√≥n | Referencia |
|:--|:-:|
| $-229.34 \left( \frac{B03}{B08} \right)^{3}+1001.65 \left( \frac{B03}{B08} \right)^{2}-1422.7 \left( \frac{B03}{B08} \right)+665.17$ | [@Ramirez2017] |
| $-244.83+40.21 \cdot B01-3.67 \cdot NDWI$ | [@Gomez2014] |

: Caracter√≠sticas principales de algoritmos tradicionales para la estimaci√≥n de s√≥lidos suspendidos. {#tbl-solsus-trad .striped .hover tbl-colwidths="[80,20]"}

::: {.block stroke='rgb("#B86092")' inset="8pt" radius="4pt"}

[Ver tabla completa en la versi√≥n online &#x2197;](https://vhgauto.quarto.pub/gistaq-parana/#tbl-solsus-trad)

:::

::::

De acuerdo a un estudio que analiz√≥ 48 cuerpos de agua, la estimaci√≥n de TSM se hizo en su mayor√≠a por modelos lineales, siendo la banda B8A la m√°s frecuente [@Cruz2023].

### M√©todos de aprendizaje autom√°tico

El **aprendizaje autom√°tico (ML)**  es una rama de la inteligencia artificial cuyo objetivo es desarrollar algoritmos capaces de resolver problemas mediante el an√°lisis de datos y la creaci√≥n de funciones que describen el comportamiento de fen√≥menos monitoreados [@Carpio2021]. Los modelos de aprendizaje autom√°tico m√°s utilizados y mencionados por los investigadores para predecir la concentraci√≥n de SST son:

* **Bosque Aleatorio (RF) y Refuerzo Adaptativo (AdB)**, modelos que se destacan por su robustez ante datos complejos y ruidosos. Estos algoritmos construyen m√∫ltiples √°rboles de decisi√≥n que analizan las relaciones entre caracter√≠sticas como el uso del suelo o el volumen de escorrent√≠a y los niveles de SST [@Moeini2021].

* **Redes Neuronales Artificiales (ANN)**, copian las redes neuronales biol√≥gicas y aprenden patrones complejos en grandes vol√∫menes de datos, como los niveles de SST en distintas condiciones ambientales [@Moeini2021],

* **k-Nearest Neighbors (kNN)**, en sus variantes de ponderaci√≥n uniforme y variable, que estima el SST en funci√≥n de la cercan√≠a en caracter√≠sticas de nuevos puntos de muestreo con datos hist√≥ricos [@Moeini2021].

El aprendizaje autom√°tico es esencial para mejorar la precisi√≥n y rapidez en el an√°lisis de la calidad del agua, proporcionando un monitoreo m√°s eficiente y menos costoso en comparaci√≥n con los m√©todos tradicionales, especialmente en √°reas de dif√≠cil acceso o con datos limitados.

:::: {.content-visible when-format="html"}

::: {.column-screen-right}

| **Modelo de machine learning** | **Software** | **Agua** | **Datos** | **M√©tricas** | **Referencias** |
|:--|:--|:--|:--|:--|:-:|
|Bagging y Random Forest|Programa R|Bah√≠a|Muestreo|Prueba de normalidad multivalente Mardia-tests y Royston|@Carpio2021|
|Regresi√≥n lineal, LASSO, regresi√≥n de vectores de soporte (SVR), K vecinos m√°s cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).|-|Lago y embalse|Sentinel-2 y UAV|$R^{2}$| @Silveira2020|
|Regresi√≥n lineal, regresi√≥n de vectores de soporte (SVR), K vecinos m√°s cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).|Programa Python|Lagos|Estaci√≥n de monitoreo (Sensores para cada par√°metro)|$R^{2}$, NSE y RMSE| @Moeini2021|

: Caracter√≠sticas principales de algoritmos de aprendizaje autom√°tico para la estimaci√≥n de s√≥lidos suspendidos. {#tbl-solsus-machine .striped .hover tbl-colwidths="[40,12,12,13,13,10]"}

:::

::::

:::: {.content-visible when-format="typst"}

| Modelo de machine learning | Referencias |
|:--|:-:|
|Bagging y Random Forest| [@Carpio2021] |
|Regresi√≥n lineal, LASSO, regresi√≥n de vectores de soporte (SVR), K vecinos m√°s cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).| [@Silveira2020] |
|Regresi√≥n lineal, regresi√≥n de vectores de soporte (SVR), K vecinos m√°s cercanos (KNN), bosque aleatorio (RF) y redes neuronales artificiales (ANN).| [@Moeini2021] |

: Caracter√≠sticas principales de algoritmos de aprendizaje autom√°tico para la estimaci√≥n de s√≥lidos suspendidos. {#tbl-solsus-machine .striped .hover tbl-colwidths="[80,20]"}

::: {.block stroke='rgb("#B86092")' inset="8pt" radius="4pt"}

[Ver tabla completa en la versi√≥n online &#x2197;](https://vhgauto.quarto.pub/gistaq-parana/#tbl-solsus-machine)

:::

::::



## Desarrollo del modelo con **Python**


### Procesamiento de datos (Sen2Cor)

Se detalla el procedimiento t√©cnico que implement√© para procesar informaci√≥n ambiental georreferenciada con el objetivo de analizar el comportamiento del par√°metro **s√≥lidos suspendidos (sol_sus)** en una regi√≥n espec√≠fica (pixel `3x3`). Para esto, utilic√© el lenguaje Python y la biblioteca `pandas`, que resulta particularmente eficiente para el manejo de estructuras tabulares. 

#### Carga de datos

Primero importo la biblioteca `pandas`, una herramienta en Python que se utiliza para manejar datos en formato tabular (como hojas de c√°lculo o CSVs). Se le da el alias `pd` por convenci√≥n, para simplificar el c√≥digo.

Luego cargo dos archivos CSV con la funci√≥n `pd.read_csv()`, la cual convierte dichos archivos en objetos del tipo `DataFrame`, que representan tablas en memoria, que son estructuras de datos similares a tablas (parecida a una hoja de Excel). Los conjuntos de datos cargados fueron:

- `gis_df`: contiene informaci√≥n geogr√°fica (latitud, longitud, pixel, etc.).
- `lab_df`: contiene datos de laboratorio, incluyendo el par√°metro de inter√©s `sol_sus`.

 Verifico la carga correcta mostrando las primeras filas con la funci√≥n `.head()`. Es √∫til para ver r√°pidamente c√≥mo es la estructura del archivo: qu√© columnas hay, qu√© tipo de datos, si se carg√≥ bien.

```{python}
import pandas as pd  # pandas es la biblioteca para manejar datos tabulares

# Cargar los archivos de datos
gis_df = pd.read_csv('datos/base_de_datos_gis.csv')
lab_df = pd.read_csv('datos/base_de_datos_lab.csv')

# Ver las primeras filas para asegurarse de que se cargaron bien
gis_df.head(), lab_df.head()

print("Primeras filas de gis_df:")
display(gis_df.head())

print("\nPrimeras filas de lab_df:")
display(lab_df.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- `pd.read_csv()` carga los archivos en estructuras llamadas *dataframes*, que funcionan como tablas.  
- `head()` te muestra las primeras 5 filas para ver c√≥mo est√°n los datos.
- `display()` permite mostrar las tablas con formato m√°s visual (en HTML).

</details>

:::

#### Filtrar el par√°metro 'sol_sus'

En el conjunto de datos del laboratorio `lab_df`, hay m√∫ltiples par√°metros medidos (como pH, turbidez, etc.). En este caso, me interesa trabajar solamente con los datos de **s√≥lidos suspendidos**, identificado como `"sol_sus"` en la columna `param`. Este filtrado selectivo lo realic√© para limitar el an√°lisis al fen√≥meno f√≠sico-qu√≠mico de inter√©s.

Filtr√© el DataFrame para quedarme solo con esas filas, y renombr√© la columna `valor` como `sol_sus` para que sea m√°s claro en los siguientes pasos.

```{python}
# Filtrar solo las filas donde el par√°metro es 'sol_sus'
sol_sus_df = lab_df[lab_df["param"] == "sol_sus"]

# Renombrar la columna 'valor' a 'sol_sus' para que tenga sentido en el merge
sol_sus_df = sol_sus_df.rename(columns={"valor": "sol_sus"})

# Mostrar para confirmar
sol_sus_df.head()
print("Primeras filas de sol_sus_df:")
display(sol_sus_df.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- `lab_df[lab_df["param"] == "sol_sus"]` filtra las filas cuyo valor en la columna `"param"` sea `"sol_sus"`.  
- `.rename(columns={"valor": "sol_sus"})` cambia el nombre de la columna `"valor"` a `"sol_sus"`.

</details>

:::


#### Transformar la columna banda en columnas individuales

En este paso, convierto los valores √∫nicos de la columna `banda` (como `B01`, `B02`, etc.) en nombres de columnas. Cada nueva columna contendr√° los valores del par√°metro `reflect` correspondientes a esa banda en particular. Esta operaci√≥n se realiza antes de unir con los valores de `sol_sus`, ya que el valor de reflectancia depende de la banda, mientras que `sol_sus` es un dato independiente que se asignar√° luego por punto, fecha y ubicaci√≥n.

```{python}
# Pivotear la tabla para que cada banda sea una columna
gis_pivot = gis_df.pivot_table(
    index=['fecha', 'punto', 'pixel', 'latitud', 'longitud'],
    columns='banda',
    values='reflect'
).reset_index()

# Eliminar el nombre del √≠ndice de columnas generado por el pivot
gis_pivot.columns.name = None

print("Primeras filas de gis_pivot:")
display(gis_pivot.head())
```

::: {.dropdown}
<details> 
<summary>üìÑ Nota t√©cnica</summary>

- `pivot_table()` reorganiza el DataFrame convirtiendo los valores de una columna (`banda`) en columnas individuales.
- `index=[...]` define las columnas que se mantendr√°n como claves (se repetir√°n por fila).
- `columns='banda'` indica qu√© columna queremos transformar en nombres de columnas.
- `values='reflect'` especifica qu√© valor colocar en cada celda de la nueva tabla.
- `reset_index()` convierte los √≠ndices jer√°rquicos en columnas normales para facilitar el an√°lisis.
- `columns.name = None` quita la etiqueta "banda" que se agregar√≠a al encabezado por defecto.

</details> 

:::


#### Combinar datos geoespaciales y de laboratorio

Una vez que las bandas han sido transformadas en columnas, combino esta tabla con los valores de s√≥lidos suspendidos (`sol_sus`) provenientes del laboratorio. La combinaci√≥n se hace usando las columnas `fecha`, `latitud` y `longitud`, que permiten identificar los datos correspondientes a un mismo punto geogr√°fico y temporal.

```{python}
# Realizar el merge por ubicaci√≥n y fecha
df_merged = pd.merge(
    gis_pivot,
    sol_sus_df[['fecha', 'latitud', 'longitud', 'sol_sus']],
    on=['fecha', 'latitud', 'longitud'],
    how='inner'
)

print("Primeras filas del DataFrame combinado:")
display(df_merged.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- `pd.merge()` permite combinar dos DataFrames en uno nuevo, uniendo filas que coincidan en las columnas especificadas.  
- `on=["latitud", "longitud"]` indica que la combinaci√≥n debe hacerse usando esas columnas como claves.  
- `how="inner"` especifica el tipo de combinaci√≥n:  
  - `"inner"`: solo conserva las filas donde hay coincidencia en ambos DataFrames.  
  - Otras opciones:  
    - `"left"`: conserva todas las filas del primer DataFrame.  
    - `"right"`: conserva todas las filas del segundo.  
    - `"outer"`: conserva todo, incluso si no hay coincidencia.

</details>

:::


#### Filtrado espacial por pixel

Luego de combinar los datos, aplico un filtrado adicional al DataFrame sobre la columna `pixel` para conservar √∫nicamente las filas correspondientes al √°rea geogr√°fica designada como `"3x3"`. Este paso reduce el dominio de an√°lisis y permite concentrarse en una regi√≥n de estudio concreta.

```{python}
# Filtrar solo los datos del pixel 3x3
df_pixel_3x3 = df_merged[df_merged["pixel"] == "3x3"]

print("Primeras filas del pixel 3x3:")
display(df_pixel_3x3.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- Usa filtrado booleano (`DataFrame[condici√≥n]`), que es la forma est√°ndar en pandas para seleccionar subconjuntos de datos. 
- `df_pixel_3x3 = df_combinado[df_combinado["pixel"] == "3x3"]` selecciona ese subconjunto. Filtra las filas cuyo valor en la columna `"pixel"` es igual a `"3x3"`.

</details>

:::


#### Guardar el archivo final

Finalmente, guardo el resultado como un nuevo archivo .csv dentro de la carpeta datos. 

Por √∫ltimo, exporto el resultado a un nuevo archivo en formato `.csv`, mediante la funci√≥n `to_csv()` de pandas, con el par√°metro `index=False` para evitar que la columna de √≠ndice se incluya en el archivo de salida que pandas crea por defecto.
Esto me permite utilizarlo despu√©s para visualizaci√≥n o an√°lisis posterior.

```{python}
# Guardar el archivo CSV dentro de la carpeta "datos"
df_pixel_3x3.to_csv('datos/datos_sol_sus_pixel_3x3.csv', index=False)
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

- `to_csv()`  guarda los datos en formato CSV.  
- `index=False` evita que se guarde el √≠ndice num√©rico del DataFrame como una columna adicional en el CSV.

</details>

:::



### Procesamiento de datos (ACOLITE)

A continuaci√≥n teniendo la misma l√≥gica de procesamiento que aplique antes, pero usando el nuevo archivo `base_de_datos_gis_acolite.csv`.

```{python}
import pandas as pd

# 1. Cargar archivos CSV 
gis_acol_df = pd.read_csv('datos/base_de_datos_gis_acolite.csv')
lab_df = pd.read_csv('datos/base_de_datos_lab.csv')

# 2. Filtrar solo el par√°metro 'sol_sus' 
sol_sus_df = lab_df[lab_df["param"] == "sol_sus"]
sol_sus_df = sol_sus_df.rename(columns={"valor": "sol_sus"})

#  3. Pivotear bandas en columnas 
gis_pivot = gis_acol_df.pivot_table(
    index=['fecha', 'punto', 'latitud', 'longitud'],
    columns='banda',
    values='reflect'
).reset_index()

gis_pivot.columns.name = None  # Quitar la etiqueta 'banda'

# 4. Combinar reflectancias con sol_sus 
df_merged = pd.merge(
    gis_pivot,
    sol_sus_df[['fecha', 'latitud', 'longitud', 'sol_sus']],
    on=['fecha', 'latitud', 'longitud'],
    how='inner'
)

# 5. Guardar resultado final
df_merged.to_csv('datos/datos_sol_sus_acolite.csv', index=False)

```



### An√°lisis de Regresi√≥n Lineal

En este an√°lisis aplico un modelo de regresi√≥n lineal simple para estudiar la relaci√≥n entre la **reflectancia** y los **s√≥lidos suspendidos**, utilizando datos experimentales. La regresi√≥n lineal es una t√©cnica fundamental del aprendizaje autom√°tico supervisado que nos permite predecir un valor continuo basado en una o m√°s variables independientes. A lo largo de este documento, se explican paso a paso las acciones realizadas y los conceptos clave para comprender y replicar este an√°lisis.


#### Importar librer√≠as

En este paso, cargo las bibliotecas necesarias para procesar datos, ajustar modelos de regresi√≥n, evaluar su desempe√±o y visualizar los resultados. 

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `pandas` se utiliza para manejar datos en forma de tablas (DataFrames), especialmente √∫tiles al trabajar con archivos `.csv`.

> `train_test_split` permite dividir los datos en subconjuntos de entrenamiento y prueba, lo cual es esencial para evaluar el desempe√±o de un modelo sin sobreajustarlo.

> `LinearRegression` representa un modelo lineal que se ajusta a los datos minimizando el error cuadr√°tico entre las predicciones y los valores reales.

> `mean_squared_error` y `r2_score` son m√©tricas de evaluaci√≥n: el primero mide el promedio de los errores al cuadrado, mientras que el segundo indica qu√© tan bien el modelo explica la variabilidad de los datos.

> `matplotlib.pyplot` se utiliza para crear gr√°ficos. Permite visualizar los datos y los resultados del modelo.
 
</details>

:::

#### Cargar datos desde un CSV

Importo el archivo `.csv` con los datos experimentales. Se visualizan las primeras filas para verificar que los datos se han cargado correctamente.

```{python}
# Cargar el CSV
datos = pd.read_csv('datos/datos_sol_sus_acolite.csv')

# Mostrar las primeras filas para verificar
datos.head()
print("Primeras filas de datos:")
display(datos.head())
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `pd.read_csv` carga datos desde un archivo `.csv` y los convierte en un DataFrame de Pandas. Esta estructura tabular permite filtrar, seleccionar y transformar f√°cilmente los datos.

> `datos.head()` permite ver las primeras 5 filas del DataFrame para tener una vista preliminar de los datos cargados.

</details>

:::

#### Seleccionar variables y dividir en conjuntos

Selecciono las variables relevantes: `B01` como variable independiente y `sol_sus` como variable dependiente. Luego divido el conjunto en dos subconjuntos: uno para entrenar el modelo y otro para probarlo, lo cual sirve para evaluar su capacidad de generalizaci√≥n.

```{python}
# Selecci√≥n de variables
X = datos[["B01"]]
y = datos["sol_sus"]

# Divisi√≥n en entrenamiento y validaci√≥n
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> Se selecciona una columna como variable independiente (X) y otra como variable dependiente (y). Es importante usar doble corchete al seleccionar una sola columna como X para mantener la estructura de tabla.

> `train_test_split` divide el conjunto de datos en entrenamiento y prueba. Esto permite entrenar el modelo en un subconjunto y evaluar su capacidad de generalizaci√≥n con otro.

> El par√°metro `test_size=0.2` indica que el 20% de los datos se usan para prueba. `shuffle=False` mantiene el orden original de los datos, √∫til cuando los datos est√°n organizados temporalmente o espacialmente.

</details>

:::


#### Entrenar modelo de regresi√≥n lineal

En este paso se entrena un modelo de regresi√≥n lineal usando los datos de entrenamiento. El modelo aprende la relaci√≥n matem√°tica entre la reflectancia y los s√≥lidos suspendidos.

```{python}
regressor = LinearRegression().fit(X_train, y_train)
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `LinearRegression().fit()` ajusta un modelo lineal a los datos de entrenamiento. Internamente calcula la pendiente e intercepto que minimizan la diferencia entre las predicciones y los valores reales.

</details>

:::


#### Evaluar desempe√±o del modelo

Una vez entrenado el modelo, evaluo su desempe√±o usando m√©tricas estad√≠sticas. Estas nos permiten cuantificar qu√© tan bien el modelo predice los valores de s√≥lidos suspendidos a partir de la reflectancia en los datos de prueba.

```{python}
y_pred = regressor.predict(X_test)
p_rmse = mean_squared_error(y_test, y_pred)
p_r2 = r2_score(y_test, y_pred)

```

::: {.callout-note title="M√©tricas de desempe√±o"}

```{python}
#| echo: false

print("El error cuadr√°tico medio es:", round(p_rmse, 3))
print("El coeficiente de determinaci√≥n (R¬≤) es:", round(p_r2, 3))
```
:::

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `predict()` genera predicciones del modelo usando los datos de prueba. Estas predicciones se comparan con los valores reales para evaluar el desempe√±o.

> `mean_squared_error` calcula el promedio de los errores al cuadrado. Cuanto menor sea este valor, mejor se ajusta el modelo.

> `r2_score` mide qu√© proporci√≥n de la variabilidad en los datos es explicada por el modelo. Un valor cercano a 1 indica una buena predicci√≥n.

</details>

:::

#### Visualizar el modelo

Finalmente, se visualiza gr√°ficamente la relaci√≥n entre reflectancia y s√≥lidos suspendidos, tanto en el conjunto de entrenamiento como en el de prueba. Esto ayuda a interpretar de forma visual c√≥mo se ajusta el modelo a los datos reales.

```{python}
fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)

# Gr√°fico entrenamiento
ax[0].plot(X_train, regressor.predict(X_train), linewidth=3, color="#17A77E", label="Modelo")
ax[0].scatter(X_train, y_train, label="Entrenamiento", color="#9D50A6", alpha=0.6)
ax[0].set(xlabel="Reflectancia", ylabel="Sol_Sus", title="Conjunto de entrenamiento")
ax[0].legend()

# Gr√°fico validaci√≥n
ax[1].plot(X_test, y_pred, linewidth=3, color="#17A77E", label="Modelo")
ax[1].scatter(X_test, y_test, label="Validaci√≥n", color="#9D50A6", alpha=0.6)
ax[1].set(xlabel="Reflectancia", ylabel="Sol_Sus", title="Conjunto de validaci√≥n")
ax[1].legend()

fig.suptitle("Regresi√≥n lineal")

plt.show()
```

::: {.dropdown}
<details>
<summary>üìÑ Nota t√©cnica</summary>

> `plt.subplots` crea una figura con uno o m√°s ejes para dibujar. Permite organizar varios gr√°ficos en una misma figura.

> `plot()` traza una l√≠nea continua. Se usa para mostrar la l√≠nea de regresi√≥n generada por el modelo.

> `scatter()` traza puntos individuales. Se usa para mostrar los datos reales y compararlos con la l√≠nea del modelo.

> `set()` configura etiquetas de ejes y t√≠tulos de los subgr√°ficos.

> `legend()` muestra una leyenda que identifica cada elemento del gr√°fico.

> `fig.suptitle()` agrega un t√≠tulo general a la figura completa.

> `plt.show()` es necesario para visualizar los gr√°ficos al renderizar el documento.

</details>

:::


### An√°lisis de regresi√≥n por banda

Con el objetivo de profundizar el an√°lisis, se eval√∫a la relaci√≥n entre los s√≥lidos suspendidos y cada una de las bandas espectrales disponibles de forma individual. Para ello, se entrena un modelo de regresi√≥n lineal simple por cada banda, utilizando los mismos datos experimentales. Este enfoque permite comparar el desempe√±o predictivo de cada banda por separado mediante m√©tricas como el coeficiente de determinaci√≥n R¬≤, su versi√≥n ajustada y el error cuadr√°tico medio (RMSE).

```{python}
#|code-fold: true
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import math

# Cargar datos
datos = pd.read_csv('datos/datos_sol_sus_acolite.csv')

# Detectar columnas de bandas
bandas = [col for col in datos.columns if col.startswith("B")]

# Lista para guardar resultados
resultados = []

# Par√°metros para organizaci√≥n de gr√°ficos
n_bandas = len(bandas)
ncols = 3  # N√∫mero de columnas de la grilla
nrows = math.ceil(n_bandas / ncols)  # Calculamos cu√°ntas filas se necesitan

# Crear figura
fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 4 * nrows))
axs = axs.flatten()  # Asegura que podamos indexarlos como una lista

for i, banda in enumerate(bandas):
    # Variables
    X = datos[[banda]]
    y = datos["sol_sus"]

    # Divisi√≥n entrenamiento/prueba
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

    # Ajuste del modelo
    modelo = LinearRegression().fit(X_train, y_train)
    y_train_pred = modelo.predict(X_train)

    # M√©tricas
    r2 = modelo.score(X_train, y_train)
    n = len(y_train)
    p = X_train.shape[1]
    r2_adj = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))

    resultados.append({
        "Banda": banda,
        "R¬≤": round(r2, 4),
        "R¬≤_ajustado": round(r2_adj, 4),
        "RMSE": round(rmse, 4)
    })

    # Gr√°fico de entrenamiento
    ax = axs[i]
    ax.scatter(X_train, y_train, alpha=0.6, color="#9D50A6", label="Entrenamiento")
    ax.plot(X_train, y_train_pred, color="#17A77E", linewidth=1.8, label="Modelo")
    ax.set_title(f'{banda}\nR¬≤={r2:.2f}', fontsize=10)
    ax.set_xlabel('Reflectancia', fontsize=8)
    ax.set_ylabel('Sol_Sus', fontsize=8)
    ax.tick_params(axis='both', which='major', labelsize=8)
    ax.legend(fontsize=7)
    ax.grid(True)

# Eliminar ejes sobrantes
for j in range(i + 1, len(axs)):
    fig.delaxes(axs[j])

plt.suptitle("Regresiones lineales por banda (entrenamiento)", fontsize=14)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# Tabla resumen
df_resultados = pd.DataFrame(resultados).sort_values("R¬≤", ascending=False).reset_index(drop=True)
print("Tabla resumen de m√©tricas por banda:")
display(df_resultados.style.hide(axis="index"))

```


::: {.callout-note title="Conclusiones del an√°lisis por banda"}

Al aplicar regresi√≥n lineal simple a cada banda espectral por separado, se observ√≥ que ninguna logra predecir con precisi√≥n los s√≥lidos suspendidos. Las bandas B05, B06 y B07 fueron las que mejor se desempe√±aron, pero con un R¬≤ apenas superior al 18‚ÄØ%, lo que indica que explican muy poca variabilidad. Adem√°s, los errores (RMSE) siguen siendo altos en relaci√≥n con los valores reales.

Esto sugiere que la relaci√≥n entre reflectancia y s√≥lidos suspendidos no es lineal en escala natural, y que ser√≠a necesario probar modelos m√°s complejos. En pr√≥ximos pasos, pienso aplicar transformaciones logar√≠tmicas y combinaciones de bandas para mejorar el ajuste.

:::



### An√°lisis de regresi√≥n por banda aplicando logaritmo a las variables

En esta etapa del an√°lisis, aplico una transformaci√≥n logar√≠tmica natural a las variables de reflectancia y s√≥lidos suspendidos antes de ajustar los modelos de regresi√≥n lineal. Esta transformaci√≥n es √∫til para:

- Estabilizar la varianza y reducir la heterocedasticidad.

- Linealizar relaciones no lineales entre variables.

- Evitar que valores extremos influyan excesivamente en el modelo.

El procedimiento es similar al an√°lisis anterior, pero antes de entrenar el modelo se aplica `log(x)` a las columnas correspondientes. Para evitar problemas con ceros en los datos, estos se reemplazan por `NaN` y se eliminan del an√°lisis.


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import math

# 1. Cargar datos 
datos = pd.read_csv("datos/datos_sol_sus_acolite.csv")
bandas = [c for c in datos.columns if c.startswith("B")]

# 2.  Filtrar positivos (requisito para log) 
mask_pos = (datos["sol_sus"] > 0) & (datos[bandas] > 0).all(axis=1)
datos_log = datos.loc[mask_pos].copy()

# 3.Transformar a log-natural (para entrenar) 
datos_log["log_sol_sus"] = np.log(datos_log["sol_sus"])
for b in bandas:
    datos_log[f"log_{b}"] = np.log(datos_log[b])

log_bandas = [f"log_{b}" for b in bandas]

# 4. Split train / test (test se descarta aqu√≠) 
X_total = datos_log[log_bandas]
y_total = datos_log["log_sol_sus"]
X_train, _, y_train, _ = train_test_split(
    X_total, y_total, test_size=0.2, shuffle=False
)

# 5. Funciones de m√©tricas 
def metrics_log(model, X, y):
    """M√©tricas sobre los datos en log (las que ya ten√≠as)."""
    y_pred = model.predict(X)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    r2   = r2_score(y, y_pred)
    n, p = X.shape
    r2_adj = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)
    return rmse, r2, r2_adj

def metrics_original(model, X, y):
    """
    Idem, pero despu√©s de volver a la escala original:
       sol_sus_pred  = exp(yÃÇ_log)
       sol_sus_true  = exp(y_log)
    """
    y_pred_log  = model.predict(X).ravel()
    y_pred_orig = np.exp(y_pred_log)
    y_true_orig = np.exp(y.values)
    rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))
    r2   = r2_score(y_true_orig, y_pred_orig)
    n, p = X.shape
    r2_adj = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)
    return rmse, r2, r2_adj

# 6.  Entrenar un modelo por banda y registrar m√©tricas 
resultados = []           # para log
resultados_nolog = []     # para escala original
lineas_log = {}           # (x, yÃÇ_log) para graficar en log-log
lineas_nolog = {}         # (x_original, yÃÇ_original) para graficar sin log

for b, lb in zip(bandas, log_bandas):
    Xtr = X_train[[lb]]
    reg = LinearRegression().fit(Xtr, y_train)

    # ‚îÄ‚îÄ M√©tricas en log
    rmse_l, r2_l, r2a_l = metrics_log(reg, Xtr, y_train)
    resultados.append({"Banda": b, "RMSE_log": rmse_l,
                       "R¬≤_log": r2_l, "R¬≤aj_log": r2a_l})

    # ‚îÄ‚îÄ M√©tricas en original
    rmse_o, r2_o, r2a_o = metrics_original(reg, Xtr, y_train)
    resultados_nolog.append({"Banda": b, "RMSE": rmse_o,
                             "R¬≤": r2_o, "R¬≤aj": r2a_o})

    # ‚îÄ‚îÄ Recta ordenada para ambas escalas
    order = np.argsort(Xtr.values.ravel())
    x_log_ord   = Xtr.values.ravel()[order]             # log(B)
    y_pred_log  = reg.predict(Xtr).ravel()[order]       # log(sol_susÃÇ)
    lineas_log[b] = (x_log_ord, y_pred_log)

    # Para volver a la escala original:
    x_orig_ord  = np.exp(x_log_ord)                     # B
    y_pred_orig = np.exp(y_pred_log)                    # sol_susÃÇ
    lineas_nolog[b] = (x_orig_ord, y_pred_orig)

# 7. DataFrames con m√©tricas
res_df_log    = (pd.DataFrame(resultados)
                   .round(3)
                   .sort_values("RMSE_log")
                   .reset_index(drop=True))

res_df_nolog  = (pd.DataFrame(resultados_nolog)
                   .round(3)
                   .sort_values("RMSE")
                   .reset_index(drop=True))

# 8.Gr√°ficos *sin* log (escala original)
n_cols = 3
n_rows = math.ceil(len(bandas) / n_cols)
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))
axes = axes.flatten()

for idx, b in enumerate(bandas):
    ax = axes[idx]
    # Datos originales de entrenamiento (sin log):
    x_orig = np.exp(X_train[f"log_{b}"])
    y_orig = np.exp(y_train)
    ax.scatter(x_orig, y_orig, alpha=0.6, color="#9D50A6", label="Entrenamiento")
    # Curva modelo en original:
    ax.plot(*lineas_nolog[b], lw=3, color="#17A77E", label="Modelo")
    ax.set(xlabel=b, ylabel="sol_sus", title=f"Banda {b} (escala original)")
    ax.legend()
    ax.grid(True)

# Si sobran ejes, quitarlos
for j in range(len(bandas), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# 9.  Mostrar tablas de m√©tricas
print("\nM√©tricas en **escala log-log** (ordenadas por RMSE_log):\n")
print(res_df_log.to_string(index=False))

print("\nM√©tricas en **escala original** (ordenadas por RMSE):\n")
print(res_df_nolog.to_string(index=False))

```

::: {.dropdown}
<details> 
<summary>üìÑ Nota t√©cnica</summary>

>La transformaci√≥n `log(x)` se usa frecuentemente cuando la relaci√≥n entre variables es multiplicativa o cuando hay asimetr√≠a en la distribuci√≥n.

>Es fundamental reemplazar ceros por NaN antes de aplicar el logaritmo, ya que `log(0)` no est√° definido.

>Las m√©tricas obtenidas (`R¬≤` y `RMSE`) se interpretan en la **escala logar√≠tmica y no son directamente** comparables con las obtenidas en la escala original.

>El `R¬≤ ajustado` penaliza la complejidad del modelo y ayuda a evaluar si el ajuste mejora m√°s all√° de lo que se esperar√≠a por azar.

</details> 

:::


::: {.callout-note title="Conclusi√≥n del ajuste log‚Äìlog por banda"}

Aplicar la transformaci√≥n logar√≠tmica mejor√≥ notablemente el ajuste respecto al an√°lisis en escala natural. Las bandas B06, B07 y B05 mostraron los mejores resultados, con R¬≤_log cercanos a 0.30 y una reducci√≥n del error (RMSE) tanto en escala logar√≠tmica como en la original.

Aun as√≠, el desempe√±o sigue siendo limitado si se usa cada banda por separado. Esto sugiere que para mejorar las predicciones es necesario combinar bandas, aplicar modelos m√°s complejos o probar con √≠ndices espectrales derivados.

:::



### Selecci√≥n de bandas con AIC y desempe√±o individual

Para evaluar qu√© bandas individuales son m√°s relevantes para predecir los s√≥lidos suspendidos, se ajust√≥ un modelo de regresi√≥n lineal simple en escala logar√≠tmica para cada banda. Adem√°s de calcular el coeficiente de determinaci√≥n (R¬≤) y el error cuadr√°tico medio (RMSE), se calcula ahora el **AIC (Criterio de Informaci√≥n de Akaike)**, que penaliza la complejidad del modelo. Esto ayuda a decidir si la incorporaci√≥n de una banda es realmente valiosa o si se est√° agregando complejidad innecesaria al modelo.


```{python}
#|code-fold: true
# 0. Librer√≠as
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 1. Cargar datos y detectar bandas
datos = pd.read_csv("datos/datos_sol_sus_acolite.csv")
bandas = [c for c in datos.columns if c.startswith("B")]

# 2. Filtrar casos con valores positivos (requisito de log)
mask_pos = (datos["sol_sus"] > 0) & (datos[bandas] > 0).all(axis=1)
datos_log = datos.loc[mask_pos].copy()

# 3. Transformar a log-natural
datos_log["log_sol_sus"] = np.log(datos_log["sol_sus"])
for b in bandas:
    datos_log[f"log_{b}"] = np.log(datos_log[b])

log_bandas = [f"log_{b}" for b in bandas]

# 4. Partici√≥n entrenamiento / test 
X_total = datos_log[log_bandas]
y_total = datos_log["log_sol_sus"]

X_train, _, y_train, _ = train_test_split(
    X_total, y_total, test_size=0.20, shuffle=False
)

# 5. Funciones auxiliares (AIC + m√©tricas)
def aic_gauss(model, X, y):
    """
    AIC para regresi√≥n lineal con ruido ~ N(0, œÉ¬≤)
    AIC = n ¬∑ ln(RSS/n) + 2k
    """
    n = len(y)
    k = X.shape[1] + 1  # predictores + intercepto
    rss = np.sum((y - model.predict(X)) ** 2)
    return n * np.log(rss / n) + 2 * k

def metrics_log(model, X, y):
    y_pred = model.predict(X)
    rmse = np.sqrt(mean_squared_error(y, y_pred))
    r2   = r2_score(y, y_pred)
    n, p = X.shape
    r2_adj = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)
    return rmse, r2, r2_adj

# 6. Ajuste univariable banda-a-banda + AIC
resultados = []

for b in bandas:
    lb = f"log_{b}"
    Xtr = X_train[[lb]]
    reg = LinearRegression().fit(Xtr, y_train)

    rmse_log, r2_log, r2a_log = metrics_log(reg, Xtr, y_train)
    aic_val = aic_gauss(reg, Xtr, y_train)

    resultados.append({
        "Banda"    : b,
        "RMSE_log" : rmse_log,
        "R¬≤_log"   : r2_log,
        "R¬≤aj_log" : r2a_log,
        "AIC"      : aic_val
    })

# 7. Tabla ordenada por AIC
df_log  = (pd.DataFrame(resultados)
             .round(3)
             .sort_values("AIC")
             .reset_index(drop=True))

print("\nAIC y m√©tricas en escala log-log (ordenadas por AIC):\n")
print(df_log.to_string(index=False))

```


::: {.callout-note title="Conclusi√≥n sobre la selecci√≥n de bandas con AIC"}

Los resultados del AIC coinciden con las m√©tricas de desempe√±o: las bandas B06 y B07 son las m√°s relevantes para predecir s√≥lidos suspendidos, mostrando los valores m√°s bajos de AIC (-69.2 y -69.1 respectivamente), as√≠ como los mejores valores de R¬≤_log (~0.30) y RMSE_log (~0.32).

Las dem√°s bandas presentan AIC menos favorables, lo que indica que, individualmente, aportan menos informaci√≥n √∫til y no justifican la complejidad del modelo.

Por lo tanto, para modelos simples basados en una sola banda, B06 y B07 son las candidatas preferentes. Sin embargo, para un mejor desempe√±o se recomienda evaluar modelos que combinen varias bandas.

:::



#### Incorporar validaci√≥n cruzada en la selecci√≥n de modelo

En esta etapa incorporo una validaci√≥n cruzada (CV) para evaluar la robustez y generalizaci√≥n del modelo univariable basado en cada banda espectral. La validaci√≥n cruzada consiste en dividir el conjunto de entrenamiento en varios ‚Äúfolds‚Äù o particiones, entrenar el modelo en todas menos una, y validar en la partici√≥n restante, repitiendo este proceso rotando las particiones. Esto ayuda a obtener m√©tricas de desempe√±o m√°s confiables y evitar sobreajuste al conjunto de entrenamiento.

El flujo completo que implemento es:
- Dividir el conjunto completo en un 80 % para entrenamiento y un 20 % para prueba final (hold-out).
- En el 80 % de entrenamiento, aplicar validaci√≥n cruzada K-Fold (5 folds) para evaluar RMSE y coeficiente de determinaci√≥n ajustado (R¬≤ ajustado) de cada banda.
- Calcular tambi√©n el AIC para cada modelo univariable, utilizando el conjunto completo de entrenamiento (80 %) para penalizar complejidad.
- Ordenar las bandas por AIC y, en caso de empate, desempatar usando el promedio de RMSE de la validaci√≥n cruzada.
- Finalmente, con la banda ganadora entrenar el modelo sobre todo el 80 % y evaluar su desempe√±o en el conjunto de prueba (hold-out 20 %).

Esta estrategia permite seleccionar la banda m√°s relevante no solo por la bondad del ajuste, sino tambi√©n considerando la estabilidad y capacidad predictiva en datos no vistos, ofreciendo una selecci√≥n m√°s robusta y realista para modelos futuros.

```{python}
#|code-fold: true
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 0. LIBRER√çAS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.utils import resample

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1. CARGAR DATOS Y DETECTAR BANDAS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
datos  = pd.read_csv("datos/datos_sol_sus_acolite.csv")
bandas = [c for c in datos.columns if c.startswith("B")]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. FILTRAR CASOS > 0  (requisito de log)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
mask_pos = (datos["sol_sus"] > 0) & (datos[bandas] > 0).all(axis=1)
datos_log = datos.loc[mask_pos].copy()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3. TRANSFORMACI√ìN LOGAR√çTMICA
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
datos_log["log_sol_sus"] = np.log(datos_log["sol_sus"])
for b in bandas:
    datos_log[f"log_{b}"] = np.log(datos_log[b])

log_bandas = [f"log_{b}" for b in bandas]

X_full = datos_log[log_bandas].values
y_full = datos_log["log_sol_sus"].values

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4. SPLIT HOLD-OUT 80/20  (shuffle=False para respetar orden)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(
    X_full, y_full, test_size=0.20, shuffle=False
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 5. FUNCIONES AUXILIARES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def aic_gauss(model, X, y):
    """AIC con supuesto de ruido gaussiano homoced√°stico."""
    n = len(y)
    k = X.shape[1] + 1    # +1 intercepto
    rss = np.sum((y - model.predict(X)) ** 2)
    return n * np.log(rss / n) + 2 * k

def r2_adj(r2, n, p):
    return 1 - (1 - r2) * (n - 1) / (n - p - 1)

def cv_metrics(X_mat, y_vec, cv):
    """Devuelve medias y sd del RMSE y medias de R¬≤ y R¬≤aj."""
    model = LinearRegression()
    rmse_folds, r2_folds, r2a_folds = [], [], []

    for tr_idx, val_idx in cv.split(X_mat):
        X_tr, X_val = X_mat[tr_idx], X_mat[val_idx]
        y_tr, y_val = y_vec[tr_idx], y_vec[val_idx]

        model.fit(X_tr, y_tr)
        y_pred = model.predict(X_val)

        rmse = np.sqrt(mean_squared_error(y_val, y_pred))
        r2   = r2_score(y_val, y_pred)
        r2a  = r2_adj(r2, len(y_val), X_mat.shape[1])

        rmse_folds.append(rmse)
        r2_folds.append(r2)
        r2a_folds.append(r2a)

    return {
        "rmse_mean": np.mean(rmse_folds),
        "rmse_std":  np.std(rmse_folds, ddof=1),
        "r2_mean":   np.mean(r2_folds),
        "r2a_mean":  np.mean(r2a_folds)
    }

# Configuraci√≥n de la validaci√≥n cruzada interior
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 6. EVALUACI√ìN UNIVARIABLE  (CV + AIC)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
resultados = []

for idx_band, b in enumerate(bandas):
    X_train_band = X_train_full[:, idx_band].reshape(-1, 1)

    # 6.1  CV
    stats_cv = cv_metrics(X_train_band, y_train_full, kf)

    # 6.2  AIC en todo el 80 %
    modelo_tmp = LinearRegression().fit(X_train_band, y_train_full)
    aic_val    = aic_gauss(modelo_tmp, X_train_band, y_train_full)

    resultados.append({
        "Banda":     b,
        "AIC_train": aic_val,
        "RMSE_CV":   stats_cv["rmse_mean"],
        "CV_std":    stats_cv["rmse_std"],
        "R2_CV":     stats_cv["r2_mean"],
        "R2aj_CV":   stats_cv["r2a_mean"]
    })

df_res = (
    pd.DataFrame(resultados)
      #   PRIMER CRITERIO: AIC  ‚ñ∏  SEGUNDO: RMSE_CV
      .sort_values(["AIC_train", "RMSE_CV"])
      .reset_index(drop=True)
      .round(4)
)

print("\nComparaci√≥n banda-a-banda (ordenada por AIC, luego RMSE_CV):\n")
print(df_res.to_string(index=False))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 7. BANDA GANADORA  ‚Üí  EVALUACI√ìN HOLD-OUT
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
best_band = df_res.loc[0, "Banda"]
idx_best  = bandas.index(best_band)

X_train_best = X_train_full[:, idx_best].reshape(-1, 1)
X_test_best  = X_test_full[:,  idx_best].reshape(-1, 1)

best_model = LinearRegression().fit(X_train_best, y_train_full)
y_pred_test = best_model.predict(X_test_best)

rmse_test = np.sqrt(mean_squared_error(y_test_full, y_pred_test))
r2_test   = r2_score(y_test_full, y_pred_test)

print(f"\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  HOLD-OUT (20 %)  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
print(f"Banda ganadora: {best_band}")
print(f"RMSE_test  = {rmse_test:.3f}")
print(f"R¬≤_test    = {r2_test:.3f}")

```


::: {.callout-note title="Conclusi√≥n sobre la selecci√≥n de bandas con AIC"}

Al incorporar validaci√≥n cruzada para evaluar la selecci√≥n univariable por banda en escala logar√≠tmica, la banda B06 se mantiene como la mejor candidata, con el AIC m√°s bajo y el RMSE promedio m√°s peque√±o en CV (aprox. 0.31).

Sin embargo, los valores de R¬≤ ajustado en CV son negativos para todas las bandas, lo que indica que el modelo lineal simple no logra explicar bien la variabilidad fuera de la muestra, sugiriendo un ajuste pobre o posible falta de linealidad real en los datos.

La evaluaci√≥n final en el conjunto hold-out (20 %) confirma esta baja capacidad predictiva, con un RMSE_test de 0.491 y un R¬≤_test negativo (-0.291), lo que indica que el modelo predice peor que un modelo que simplemente promedie los datos.

En resumen, aunque B06 es la banda m√°s prometedora seg√∫n los criterios AIC y RMSE en entrenamiento y validaci√≥n interna, la baja performance en datos nuevos apunta a la necesidad de modelos m√°s complejos, combinaciones multivariables o transformaciones adicionales para mejorar la predicci√≥n.

:::



### Selecci√≥n de bandas con Forward Selection basado en AIC

En esta etapa realizo una selecci√≥n de variables (bandas espectrales) combinadas utilizando un m√©todo de **Forward Selection** guiado por el criterio de informaci√≥n de Akaike (AIC). Este m√©todo comienza con un modelo vac√≠o e incorpora progresivamente la banda que produce la mayor mejora (reducci√≥n) en el AIC, deteni√©ndose cuando no se consigue una mejora significativa.

La selecci√≥n se realiza sobre datos transformados en escala logar√≠tmica para estabilizar varianzas y linealizar relaciones. Adem√°s, se valida el desempe√±o del modelo en cada paso mediante validaci√≥n cruzada (5 folds), evaluando m√©tricas como RMSE y R¬≤ en escala original para tener una idea m√°s realista del ajuste.

Finalmente, se eval√∫a el modelo seleccionado en un conjunto independiente de test (20 % de los datos) para estimar su capacidad predictiva fuera de muestra. Tambi√©n se presenta la ecuaci√≥n final en escala log-log y en escala original para facilitar la interpretaci√≥n.


```{python}
#|code-fold: true
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 0 ¬∑ LIBRER√çAS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1 ¬∑ CARGAR Y PRE-PROCESAR
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
df     = pd.read_csv("datos/datos_sol_sus_acolite.csv")
bandas = [c for c in df.columns if c.startswith("B")]

mask   = (df["sol_sus"] > 0) & (df[bandas] > 0).all(axis=1)
df     = df.loc[mask].copy()

df["log_sol_sus"] = np.log(df["sol_sus"])
for b in bandas:
    df[f"log_{b}"] = np.log(df[b])

log_cols = [f"log_{b}" for b in bandas]
X_full   = df[log_cols].values
y_full   = df["log_sol_sus"].values          # todav√≠a en log

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2 ¬∑ HOLD-OUT 80 / 20  (orden intacto ‚Üí shuffle=False)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
X_tr, X_te, y_tr, y_te = train_test_split(
    X_full, y_full, test_size=0.20, shuffle=False
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3 ¬∑ FUNCIONES AUXILIARES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def aic_gauss(model, X, y):
    n = len(y)
    k = X.shape[1] + 1
    rss = np.sum((y - model.predict(X))**2)
    return n * np.log(rss / n) + 2 * k

def r2_adj(r2, n, p):
    return 1 - (1 - r2) * (n - 1) / (n - p - 1)

def cv_metrics(Xm, ym, cv):
    """
    Devuelve m√©tricas en ESCALA ORIGINAL para 5-fold CV:
      ¬∑ rmse_mean, rmse_std, r2_mean, r2a_mean
    """
    model = LinearRegression()
    rmse_f, r2_f = [], []

    for tr_idx, vl_idx in cv.split(Xm):
        model.fit(Xm[tr_idx], ym[tr_idx])          # log-modelo
        pred_log = model.predict(Xm[vl_idx])

        # ‚Üí escala original
        y_val_orig   = np.exp(ym[vl_idx])
        pred_orig    = np.exp(pred_log)

        rmse_f.append(np.sqrt(mean_squared_error(y_val_orig, pred_orig)))
        r2_f  .append(r2_score(y_val_orig, pred_orig))

    r2_mean = np.mean(r2_f)
    r2a_mean = r2_adj(r2_mean, len(ym), Xm.shape[1])
    return {
        "rmse_mean": np.mean(rmse_f),
        "rmse_std":  np.std(rmse_f, ddof=1),
        "r2_mean":   r2_mean,
        "r2a_mean":  r2a_mean
    }

kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4 ¬∑ FORWARD-SELECTION GUIADO POR AIC
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
remain, select, best_aic, hist = list(range(len(bandas))), [], np.inf, []

while remain:
    best_cand = None
    for idx in remain:
        cols = select + [idx]
        mdl  = LinearRegression().fit(X_tr[:, cols], y_tr)
        aic  = aic_gauss(mdl, X_tr[:, cols], y_tr)
        if best_cand is None or aic < best_cand[0]:
            best_cand = (aic, idx)

    # mejora ‚â• 2 unidades
    if best_cand[0] + 2 < best_aic:
        best_aic = best_cand[0]
        select.append(best_cand[1])
        remain.remove(best_cand[1])

        stats = cv_metrics(X_tr[:, select], y_tr, kf)
        hist.append({
            "Paso": len(select),
            "A√±adido": bandas[best_cand[1]],
            "AIC": round(best_aic, 3),
            "RMSE_CV": round(stats["rmse_mean"], 3),
            "CV_std":  round(stats["rmse_std"], 3),
            "R2_CV":   round(stats["r2_mean"], 3),
            "R2aj_CV": round(stats["r2a_mean"], 3)
        })
    else:
        break

hist_df = pd.DataFrame(hist)
print("\nHistorial forward-AIC:")
print(hist_df)

sel_b = [bandas[i] for i in select]
print(f"\nBandas finales seleccionadas: {sel_b}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 5 ¬∑ AJUSTE FINAL Y M√âTRICAS TEST (ORIGINAL)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Xtr_sel = X_tr[:, select]
Xte_sel = X_te[:, select]

final_model = LinearRegression().fit(Xtr_sel, y_tr)

# ‚Üí escala original para test
pred_test_orig = np.exp(final_model.predict(Xte_sel))
y_test_orig    = np.exp(y_te)

rmse_test = np.sqrt(mean_squared_error(y_test_orig, pred_test_orig))
mae_test  = mean_absolute_error(y_test_orig, pred_test_orig)
r2_test   = r2_score(y_test_orig, pred_test_orig)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 6 ¬∑ ECUACI√ìN FINAL (formato fracci√≥n cuando hay exponentes negativos)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
interc  = final_model.intercept_
coefs   = final_model.coef_

pos_terms = [f"{b}^{c:.4f}"      for b, c in zip(sel_b, coefs) if c >= 0]
neg_terms = [f"{b}^{-c:.4f}"     for b, c in zip(sel_b, coefs) if c < 0]

k_factor  = math.exp(interc)
numerator = " * ".join(pos_terms) if pos_terms else "1"
denominator = " * ".join(neg_terms)

print("\nEcuaci√≥n en escala log-log:")
log_eq = [f"{interc:+.4f}"] + [
    f"{coefs[i]:+0.4f}¬∑log({sel_b[i]})" for i in range(len(sel_b))
]
print("log(sol_sus) = " + " ".join(log_eq))

print("\nEscala original:")
if denominator:
    print(f"sol_sus_orig = {k_factor:.2f} ¬∑ ({numerator}) / ({denominator})")
else:
    print(f"sol_sus_orig = {k_factor:.2f} ¬∑ {numerator}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 7 ¬∑ TABLA FINAL DE M√âTRICAS
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
final_row = {
    "Bandas": "+".join(sel_b),
    "AIC_train": round(best_aic, 3),
    "RMSE_CV":   round(hist[-1]["RMSE_CV"], 3),
    "CV_std":    round(hist[-1]["CV_std"], 3),
    "R¬≤_CV":     round(hist[-1]["R2_CV"], 3),
    "R¬≤aj_CV":   round(hist[-1]["R2aj_CV"], 3),
    "RMSE_test": round(rmse_test, 3),
    "MAE_test":  round(mae_test, 3),
    "R¬≤_test":   round(r2_test, 3)
}

print("\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ TABLA FINAL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
print(pd.DataFrame([final_row]).to_string(index=False))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 8 ¬∑ GR√ÅFICO REAL vs PREDICHO (train + test)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Train ‚Üí original
pred_train_orig = np.exp(final_model.predict(Xtr_sel))
y_train_orig    = np.exp(y_tr)

plt.figure(figsize=(6, 6))
plt.scatter(y_train_orig, pred_train_orig,
            alpha=0.7, label="Train")
plt.scatter(y_test_orig, pred_test_orig,
            alpha=0.7, marker='s', edgecolors='k', label="Test")

# l√≠nea y = x
mn, mx = min(y_train_orig.min(), y_test_orig.min()), \
         max(y_train_orig.max(), y_test_orig.max())
plt.plot([mn, mx], [mn, mx], "--", color="gray")

plt.xlabel("S√≥lidos suspendidos observados")
plt.ylabel("S√≥lidos suspendidos predichos")
plt.title("Ajuste modelo (escala original)")

# ecuaci√≥n en caja
eq_line = rf"$\hat{{y}} = {k_factor:.1f}"
for b, c in zip(sel_b, coefs):
    eq_line += rf"\times {b}^{{{c:.2f}}}"
eq_line += "$"
plt.text(0.05, 0.95, eq_line,
         transform=plt.gca().transAxes,
         fontsize=8, va="top",
         bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", lw=0.5))

plt.legend()
plt.tight_layout()
plt.show()

```


::: {.callout-note title="Conclusi√≥n del modelo seleccionado"}

El algoritmo de Forward Selection identific√≥ a las bandas B06 y B02 como las m√°s relevantes para predecir los s√≥lidos suspendidos en escala log-log. El modelo final presenta un buen desempe√±o, con un RMSE_test de 16.69 y un R¬≤_test de 0.86 en escala original, lo que indica una capacidad predictiva adecuada sobre datos no vistos. Adem√°s, la ecuaci√≥n obtenida permite expresar la variable objetivo en funci√≥n de las bandas seleccionadas de forma clara y directa.

Aunque el ajuste es satisfactorio, ser√≠a conveniente en futuros an√°lisis explorar modelos no lineales o t√©cnicas multivariadas m√°s complejas, como regresi√≥n regularizada o √°rboles de decisi√≥n, que podr√≠an captar relaciones m√°s sutiles entre variables. 

:::
